
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12. Chapter 10: Inference for Regression &#8212; STAT301@Purdue</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter12';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. Chapter 11: Multiple Regression" href="chapter13.html" />
    <link rel="prev" title="11. Chapter 2: Least Squares Regression" href="chapter11.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/statlogo.png" class="logo__image only-light" alt="STAT301@Purdue - Home"/>
    <img src="_static/statlogo.png" class="logo__image only-dark pst-js-only" alt="STAT301@Purdue - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    STAT 301 Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">At the beginning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">1. At the Beginning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Working with data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">2. Some Important Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">3. Chapter 3: Producing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">4. Chapter 1: Looking at Data – Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Making statistical inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">5. Chapter 5: Sampling Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">6. Chapter 6: Confidence Intervals and Significance Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">7. Chapter 7: Inference for Means</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">8. Chapter 12: One Way ANOVA</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">9. Chapter 13: Two Way ANOVA</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling linear regression</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">10. Chapter 2: Looking at Data-Relationships</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">11. Chapter 2: Least Squares Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Chapter 10: Inference for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">13. Chapter 11: Multiple Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter12.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter12.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 10: Inference for Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">12.1. Simple Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-parameters">12.2. Estimating the regression parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-significant-tests">12.3. Confidence intervals and significant tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-detail-about-simple-linear-regression">12.4. More detail about simple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-errors">12.5. Standard Errors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details-on-standard-errors-of-mean-response-and-prediction">12.6. More details on standard errors of mean response and prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-mathrm-var-hat-mu-y-0">12.6.1. Deriving <span class="math notranslate nohighlight">\( \mathrm{Var}(\hat{\mu}_{y_0}) \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-observation-prediction-interval">12.6.2. Future Observation (Prediction Interval)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-10-inference-for-regression">
<h1><span class="section-number">12. </span>Chapter 10: Inference for Regression<a class="headerlink" href="#chapter-10-inference-for-regression" title="Link to this heading">#</a></h1>
<p>In this chapter, we will study the <span style="color:#cfb991"><strong>inference for regression</strong></span>. In the previous chapter, when we had one sample—one dataset—we could plot a regression line in that scatterplot. But what if we have a different sample, a different dataset from the population? We might end up with a different regression line.</p>
<p>The values <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> are our estimates of the true population parameters of the linear model, which are <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. Just as we use <span class="math notranslate nohighlight">\(\bar{x}\)</span> to estimate <span class="math notranslate nohighlight">\(\mu\)</span>, we need to calculate the standard errors of our estimates to quantify the sampling variability of our estimates and to conduct similar statistical inference procedures.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Understanding Uncertainty in Regression Estimates</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">For one scatterplot, we can have a single line given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = b_0 + b_1 x
\]</div>
<p class="sd-card-text">We use <span class="math notranslate nohighlight">\(b_0\)</span> to denote <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, and <span class="math notranslate nohighlight">\(b_1\)</span> to denote <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>. This fitted line is fixed for that particular dataset. However, if we obtain a different dataset, which produces a different scatterplot, the pair of estimates <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> is likely to change.</p>
<p class="sd-card-text">The <strong>standard errors</strong> associated with <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> quantify the uncertainty in these estimates. They measure how much the estimates would vary if we repeated the sampling process and refitted the model each time.</p>
<ul class="simple">
<li><p class="sd-card-text">For one scatterplot, you have one fixed line with estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p></li>
<li><p class="sd-card-text">With a different dataset, these estimates would likely change due to sampling variability.</p></li>
<li><p class="sd-card-text">The standard errors for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> reflect this uncertainty and help quantify the variability of the estimated regression line across different samples.</p></li>
</ul>
<blockquote>
<div><p class="sd-card-text">Statistical inference is fundamentally about quantifying the uncertainty in our estimates due to sampling variability. When we draw a sample from a population, our estimates (like <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> in a regression model) can vary from one sample to another. Statistical inference provides tools—such as standard errors, confidence intervals, and hypothesis tests—to measure and account for this variability, allowing us to make probabilistic statements about the true population parameters despite the uncertainty inherent in any single sample. <a class="footnote-reference brackets" href="#footnote01" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
</div></blockquote>
</div>
</details><section id="simple-linear-regression">
<h2><span class="section-number">12.1. </span>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Link to this heading">#</a></h2>
<p>Having this sampling variability in mind, we can imagine observing all the data points—that is, every pair of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values—in the population. We also assume that <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> share a linear relationship, which can be expressed by the simple linear regression model (called “simple” because it includes only one independent variable):</p>
<div class="note admonition">
<p class="admonition-title"><strong>Simple Linear Regression Model</strong></p>
<ul>
<li><p>Given <span class="math notranslate nohighlight">\(N\)</span> observations of <span class="math notranslate nohighlight">\((x, y)\)</span> pairs:</p>
<div class="math notranslate nohighlight">
\[
  (x_1, y_1), (x_2, y_2), ..., (x_N, y_N)
  \]</div>
</li>
<li><p>The response variable follows:</p>
<div class="math notranslate nohighlight">
\[
  y_i = \beta_0 + \beta_1x_i + \epsilon_i
  \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>: <strong>Intercept</strong>, <span class="math notranslate nohighlight">\(\beta_1\)</span>: <strong>Slope</strong>, <span class="math notranslate nohighlight">\(\sigma\)</span>: <strong>Regression standard deviation</strong>.</p></li>
<li><p>Error term, <span class="math notranslate nohighlight">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span></p></li>
<li><p>Equation: Data = Fit + Residual</p></li>
<li><p>Residuals: Differences between observed and predicted <span class="math notranslate nohighlight">\(y\)</span> values.</p></li>
</ul>
</div>
<p>Again, we assume that <span class="math notranslate nohighlight">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span> because not all data points lie exactly on the line; this term represents the deviation from the line. Additionally, this implies that if we condition on each <span class="math notranslate nohighlight">\(x_i\)</span> (as illustrated by the vertical blue dashed lines in the scatterplot from the last chapter), then by taking the average of the corresponding <span class="math notranslate nohighlight">\(y\)</span> values, we obtain:</p>
<div class="math notranslate nohighlight">
\[
\mu_{y_i} = \mathbb{E}(y_i \mid x_i) = \mathbb{E}(\beta_0 + \beta_1x_i + \epsilon_i \mid x_i) = \beta_0 + \beta_1x_i + \mathbb{E}(\epsilon_i)  = \beta_0 + \beta_1x_i
\]</div>
<p>or,</p>
<div class="math notranslate nohighlight">
\[
\mu_{y} = \beta_0 + \beta_1x
\]</div>
<p>We can visualize this <strong>population regression line</strong> with the figure below:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/1201.png"><img alt="population regression line" src="_images/1201.png" style="width: 100%;" /></a>
</figure>
<ul class="simple">
<li><p>The <strong>explanatory variable</strong> <span class="math notranslate nohighlight">\(x\)</span> can have multiple values (e.g., different doses of calcium supplement).</p></li>
<li><p>Each value of <span class="math notranslate nohighlight">\(x\)</span> defines a <strong>subpopulation</strong>, assumed to have a <strong>Normally distributed</strong> response <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>The <strong>population regression line</strong> describes the mean response <span class="math notranslate nohighlight">\(\mu_y\)</span> as a function of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>All subpopulations have the <strong>same spread</strong>, measured by <strong>standard deviation</strong> <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p><strong>Key Assumptions:</strong></p>
<ul>
<li><p>The mean response <strong><span class="math notranslate nohighlight">\(\mu_y\)</span> changes</strong> as <span class="math notranslate nohighlight">\(x\)</span> changes, forming a <strong>straight-line pattern</strong>.</p></li>
<li><p>Individual responses <span class="math notranslate nohighlight">\(y\)</span> vary <strong>Normally</strong> within each subpopulation.</p></li>
<li><p>Standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is <strong>constant</strong> for all values of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
</li>
</ul>
<p>In reality, we cannot observe all the data points for each subpopulation; we only have a sample of data points for them. Thus, we can only obtain the estimates, <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>, for the true parameters, <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, respectively.</p>
</section>
<section id="estimating-the-regression-parameters">
<h2><span class="section-number">12.2. </span>Estimating the regression parameters<a class="headerlink" href="#estimating-the-regression-parameters" title="Link to this heading">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Estimating the Regression Parameters</label><div class="sd-tab-content docutils">
<p>The method of least squares presented in Chapter 2 fits a line to summarize a relationship between the observed values of an explanatory variable and a response variable. Now we want to use the least-squares regression line as a basis for inference about a population from which our observations are a sample. In that setting, the slope <span class="math notranslate nohighlight">\(b_1\)</span> and intercept <span class="math notranslate nohighlight">\(b_0\)</span> of the least-squares line</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b_0 + b_1\,x\]</div>
<p>estimate the slope <span class="math notranslate nohighlight">\(\beta_1\)</span> and the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> of the <strong>population regression line</strong>.</p>
<div class="warning admonition">
<p class="admonition-title">Important</p>
<p><em>This inference should only be done when the linear regression model conditions are reasonably met.</em></p>
</div>
<p>Various checks are needed, and some judgment is required. Because additional methods to check these conditions rely on first fitting the model to the data, let’s briefly review the estimation methods of Chapter 2.</p>
<p>Using the formulas from Chapter 2, the slope of the least-squares line is:</p>
<div class="math notranslate nohighlight">
\[b_1 = r \,\frac{s_y}{s_x}\]</div>
<p>and the intercept is:</p>
<div class="math notranslate nohighlight">
\[b_0 = \bar{y} \;-\; b_1\,\bar{x}.\]</div>
<p>Here, <span class="math notranslate nohighlight">\(r\)</span> is the correlation between the observed values of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(s_y\)</span> is the standard deviation of the sample <span class="math notranslate nohighlight">\(y\)</span>’s, and <span class="math notranslate nohighlight">\(s_x\)</span> is the standard deviation of the sample <span class="math notranslate nohighlight">\(x\)</span>’s.</p>
<p>The <strong>residuals</strong> <span class="math notranslate nohighlight">\(e_i\)</span> correspond to the linear regression model deviations <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>. The <span class="math notranslate nohighlight">\(e_i\)</span> sum to 0, and the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> come from a population with mean 0. Because we do not observe the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, we use the residuals to estimate <span class="math notranslate nohighlight">\(\sigma\)</span> and check the model conditions of the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<div class="math notranslate nohighlight">
\[e_i = \text{observed response} - \text{predicted response}
     = y_i - \hat{y}_i
     = y_i - b_0 - b_1\,x_i.\]</div>
<p>Recall also that the least-squares regression line always passes through the point <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>. Note that if the slope <span class="math notranslate nohighlight">\(b_1\)</span> is 0, so is the correlation <span class="math notranslate nohighlight">\(r\)</span>, and vice versa.</p>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
Regression Standard Error and Model Standard Deviation</label><div class="sd-tab-content docutils">
<p>To estimate <span class="math notranslate nohighlight">\(\sigma\)</span>, which measures the variation of <span class="math notranslate nohighlight">\(y\)</span> about the population regression line, we look at the <strong>average squared residual</strong> (in simple linear regression). The sample of deviations is formed by the residuals <span class="math notranslate nohighlight">\(e_i\)</span>, which stand in for the unobserved <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>.</p>
<p>We define:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{\sum e_i^2}{n - 2} 
     = \frac{\sum (y_i - \hat{y}_i)^2}{n - 2},\]</div>
<p>where <span class="math notranslate nohighlight">\(n - 2\)</span> is the <strong>degrees of freedom</strong> for this estimate. Taking the square root of <span class="math notranslate nohighlight">\(s^2\)</span> gives us:</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{s^2},\]</div>
<p>the <strong>regression standard error</strong>. It is our estimate of the model standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Estimating the Regression Parameters</p>
<p>In the simple linear regression setting, we use the slope <span class="math notranslate nohighlight">\(b_1\)</span> and intercept <span class="math notranslate nohighlight">\(b_0\)</span> of the least-squares regression line to estimate the slope <span class="math notranslate nohighlight">\(\beta_1\)</span> and intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> of the population regression line, respectively.</p>
<p>The standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> in the model is estimated by the <strong>regression standard error</strong>:</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{ \frac{1}{n - 2} \,\sum (y_i - \hat{y}_i)^2 }.\]</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
Linear Regression Model Conditions</label><div class="sd-tab-content docutils">
<div class="note admonition">
<p class="admonition-title">Model Conditions vs. <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x\)</span> Distributions</p>
<p><em>It is a common mistake to assess the Normality of <span class="math notranslate nohighlight">\(y\)</span> when checking model conditions. Even though examining the distributions of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> can help identify outliers or influential observations, the key requirement is that the <strong>model deviations</strong> (the residuals) are approximately Normal, not necessarily <span class="math notranslate nohighlight">\(x\)</span> or <span class="math notranslate nohighlight">\(y\)</span> individually.</em></p>
</div>
<div class="important admonition">
<p class="admonition-title">Linear Regression Model Conditions</p>
<p>To use the least-squares regression line as a basis for inference about a population, each of the following conditions should be <strong>approximately met</strong>:</p>
<ul class="simple">
<li><p>The sample is an SRS from the population.</p></li>
<li><p>There is a linear relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>The standard deviation of the responses <span class="math notranslate nohighlight">\(y\)</span> about the population regression line is the same for all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>The model deviations are Normally distributed.</p></li>
</ul>
</div>
<p>In settings where the choices of <span class="math notranslate nohighlight">\(x\)</span> are controlled — for example, assigning subjects different milligrams of a calcium supplement — we consider the subjects to be an SRS from the population, and that they are randomly assigned to the different choices of <span class="math notranslate nohighlight">\(x\)</span>. In other words, the <span class="math notranslate nohighlight">\(y\)</span>’s for each value of <span class="math notranslate nohighlight">\(x\)</span> are an SRS from its subpopulation.</p>
<p>Provided our check of conditions gives no reason to question the use of the simple linear regression model, we can proceed to inference about:</p>
<ul class="simple">
<li><p>The slope <span class="math notranslate nohighlight">\(\beta_1\)</span> and the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> of the population regression line.</p></li>
<li><p>The mean response <span class="math notranslate nohighlight">\(\mu_y\)</span> for a given value of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>An individual future response <span class="math notranslate nohighlight">\(y\)</span> for a given value of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="confidence-intervals-and-significant-tests">
<h2><span class="section-number">12.3. </span>Confidence intervals and significant tests<a class="headerlink" href="#confidence-intervals-and-significant-tests" title="Link to this heading">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Confidence Intervals and Significance Tests</label><div class="sd-tab-content docutils">
<p>Chapter 7 presented confidence intervals and significance tests for means and differences in means. In each case, inference rested on the standard errors of estimates and on <span class="math notranslate nohighlight">\(t\)</span> distributions. Inference in simple linear regression is similar in principle. For example, the confidence intervals have the form:</p>
<div class="math notranslate nohighlight">
\[\text{estimate} \;\pm\; t^* \cdot \text{SE}_{\text{estimate}},\]</div>
<p>where <span class="math notranslate nohighlight">\(t^*\)</span> is a critical point of a <span class="math notranslate nohighlight">\(t\)</span> distribution. It is only the formulas for the estimate and standard error that are different.</p>
<p>As a consequence of the model assumptions about the deviations <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, the sampling distributions of <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> are Normally distributed with means <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> and standard deviations that are multiples of <span class="math notranslate nohighlight">\(\sigma\)</span>, the model parameter that describes the variability about the true regression line. In fact, even if the <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> are not Normally distributed, a general form of the central limit theorem tells us that the distributions of <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> will be approximately Normal.</p>
<p>Because we do not know <span class="math notranslate nohighlight">\(\sigma\)</span>, we use the estimated model standard deviation <span class="math notranslate nohighlight">\(s\)</span>, which measures the variability of the data about the least-squares line. When we do this, we again move from the Normal distribution to <span class="math notranslate nohighlight">\(t\)</span> distributions but now with degrees of freedom <span class="math notranslate nohighlight">\(n - 2\)</span>, the degrees of freedom of <span class="math notranslate nohighlight">\(s\)</span>. We give formulas for the standard errors <span class="math notranslate nohighlight">\(\text{SE}_{b_1}\)</span> and <span class="math notranslate nohighlight">\(\text{SE}_{b_0}\)</span> in Section 10.2 of the textbook.</p>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
Inference for the Regression Slope</label><div class="sd-tab-content docutils">
<div class="tip admonition">
<p class="admonition-title">INFERENCE FOR THE REGRESSION SLOPE</p>
<p>A level <span class="math notranslate nohighlight">\(C\)</span> <strong>confidence interval for the slope</strong> <span class="math notranslate nohighlight">\(\beta_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[b_1 \;\pm\; t^* \cdot \text{SE}_{b_1},\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(t^*\)</span> is the value for the <span class="math notranslate nohighlight">\(t(n - 2)\)</span> density curve with area <span class="math notranslate nohighlight">\(C\)</span> between <span class="math notranslate nohighlight">\(-t^*\)</span> and <span class="math notranslate nohighlight">\(t^*\)</span>. The margin of error is <span class="math notranslate nohighlight">\(m = t^* \cdot \text{SE}_{b_1}\)</span>.</p>
<p>To test the hypothesis <span class="math notranslate nohighlight">\(H_0\!: \beta_1 = \beta_1^*\)</span>, we use the <strong>test statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[t = \frac{b_1 - \beta_1^*}{\text{SE}_{b_1}}.\]</div>
<p>If <span class="math notranslate nohighlight">\(H_0\!: \beta_1 = 0\)</span>, then the test statistic reduces to:</p>
<div class="math notranslate nohighlight">
\[t = \frac{b_1}{\text{SE}_{b_1}}.\]</div>
<p>With degrees of freedom <span class="math notranslate nohighlight">\(n - 2\)</span>, the <span class="math notranslate nohighlight">\(P\)</span>-value for the hypothesis depends on the alternative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H_a: \beta_1 &gt; \beta_1^* &amp; \;\Rightarrow\; P(T \ge t), \\
H_a: \beta_1 &lt; \beta_1^* &amp; \;\Rightarrow\; P(T \le t), \\
H_a: \beta_1 \ne \beta_1^* &amp; \;\Rightarrow\; 2\,P(T \ge |t|).
\end{aligned}\end{split}\]</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/1202.png"><img alt="Inference for the Regression Slope" src="_images/1202.png" style="width: 80%;" /></a>
</figure>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-5">
Testing the Intercept and the Slope</label><div class="sd-tab-content docutils">
<p>Formulas for confidence intervals and significance tests for the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> are exactly the same, replacing <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(\text{SE}_{b_1}\)</span> by <span class="math notranslate nohighlight">\(b_0\)</span> and its standard error <span class="math notranslate nohighlight">\(\text{SE}_{b_0}\)</span>.</p>
<p>Although computer outputs often include a test of <span class="math notranslate nohighlight">\(H_0: \beta_0 = 0\)</span>, this usually has little practical value. From the equation <span class="math notranslate nohighlight">\(\mu_y = \beta_0 + \beta_1 x\)</span>, we see that <span class="math notranslate nohighlight">\(\beta_0\)</span> is the mean response when <span class="math notranslate nohighlight">\(x = 0\)</span>, which is often not meaningful.</p>
<p>The test of <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> is more useful. When <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>, the model becomes:</p>
<div class="math notranslate nohighlight">
\[\mu_y = \beta_0.\]</div>
<p>This implies that the mean of <span class="math notranslate nohighlight">\(y\)</span> does not vary with <span class="math notranslate nohighlight">\(x\)</span>. All <span class="math notranslate nohighlight">\(y\)</span> values come from a population with mean <span class="math notranslate nohighlight">\(\beta_0\)</span> (estimated by <span class="math notranslate nohighlight">\(\bar{y}\)</span>), and there is <em>no straight-line relationship</em> between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Interpreting P-values:</p>
<p>Note, a large <span class="math notranslate nohighlight">\(t\)</span> value however, that this is not the same as concluding that we have found a <strong>strong</strong> linear relationship. As seen in Figure 10.3 in the textbook, there can be substantial scatter.</p>
<div class="note admonition">
<p class="admonition-title">Important</p>
<p>A very small <span class="math notranslate nohighlight">\(P\)</span>-value for the significance test for a zero slope does <strong>not</strong> necessarily imply that we have found a strong relationship.</p>
</div>
<p>A confidence interval provides more information. In software, these intervals may be optional and must be requested. They can also be constructed by hand.</p>
</div>
<input id="sd-tab-item-6" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-6">
Confidence Intervals for Mean Response</label><div class="sd-tab-content docutils">
<p>Besides slope and intercept, we may want to use the regression line to estimate the mean response <span class="math notranslate nohighlight">\(y\)</span> at a specific <span class="math notranslate nohighlight">\(x = x^*\)</span>. The mean of the response is:</p>
<div class="math notranslate nohighlight">
\[\mu_y = \beta_0 + \beta_1\,x^*.\]</div>
<p>The estimate is:</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}_y = b_0 + b_1\,x^*.\]</div>
<div class="tip admonition">
<p class="admonition-title">CONFIDENCE INTERVAL FOR A MEAN RESPONSE</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}_y \;\pm\; t^* \cdot \text{SE}_{\hat{\mu}_y},\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(t^*\)</span> is from the <span class="math notranslate nohighlight">\(t(n - 2)\)</span> distribution and <span class="math notranslate nohighlight">\(m = t^* \cdot \text{SE}_{\hat{\mu}_y}\)</span>.</p>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-7">
Prediction Intervals</label><div class="sd-tab-content docutils">
<p>To predict an <em>individual</em> observation <span class="math notranslate nohighlight">\(y\)</span> for <span class="math notranslate nohighlight">\(x = x^*\)</span>, we again use:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b_0 + b_1\,x^*.\]</div>
<p>This looks like the same formula as for <span class="math notranslate nohighlight">\(\hat{\mu}_y\)</span>, but here it predicts a single future value. We use different notation to emphasize:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> for <strong>future value</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\mu}_y\)</span> for <strong>population mean</strong>.</p></li>
</ul>
<p>For example, predicted BMI <span class="math notranslate nohighlight">\(= 23.7\ \text{kg/m}^2\)</span>. But a useful prediction also needs a margin of error.</p>
<p>A <strong>prediction interval</strong> is used to estimate where a future observation will fall. It is wider than the confidence interval because it includes both:</p>
<ul class="simple">
<li><p>variability around the regression line,</p></li>
<li><p>variability of future observations.</p></li>
</ul>
<p>Repeat:</p>
<ul class="simple">
<li><p>Sample <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and one more <span class="math notranslate nohighlight">\((x^*, y)\)</span>.</p></li>
<li><p>Construct a 95% prediction interval for <span class="math notranslate nohighlight">\(y\)</span> using <span class="math notranslate nohighlight">\(x^*\)</span>.</p></li>
</ul>
<p>Prediction Interval for a Future Observation:</p>
<div class="tip admonition">
<p class="admonition-title">PREDICTION INTERVAL FOR A FUTURE OBSERVATION</p>
<p>A level <span class="math notranslate nohighlight">\(C\)</span> <strong>prediction interval</strong> for a future observation <span class="math notranslate nohighlight">\(y\)</span> from the subpopulation corresponding to <span class="math notranslate nohighlight">\(x^*\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\hat{y} \;\pm\; t^* \cdot \text{SE}_{\hat{y}},\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(t^*\)</span> comes from <span class="math notranslate nohighlight">\(t(n - 2)\)</span> with area <span class="math notranslate nohighlight">\(C\)</span> between <span class="math notranslate nohighlight">\(-t^*\)</span> and <span class="math notranslate nohighlight">\(t^*\)</span>.<br />
The margin of error is <span class="math notranslate nohighlight">\(m = t^* \cdot \text{SE}_{\hat{y}}\)</span>.</p>
<p>Then, 95% of such intervals will capture the true <span class="math notranslate nohighlight">\(y\)</span> for a future observation. The formula for <span class="math notranslate nohighlight">\(\text{SE}_{\hat{y}}\)</span> includes both the uncertainty of estimating the line and variability around the mean.</p>
</div>
</div>
</section>
<section id="more-detail-about-simple-linear-regression">
<h2><span class="section-number">12.4. </span>More detail about simple linear regression<a class="headerlink" href="#more-detail-about-simple-linear-regression" title="Link to this heading">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-8" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-8">
Analysis of Variance for Regression</label><div class="sd-tab-content docutils">
<p>The usual computer output for regression includes an additional block of calculations labeled <strong>“ANOVA”</strong> or <strong>“Analysis of Variance.”</strong><br />
<strong>Analysis of variance</strong> (often abbreviated ANOVA) refers to statistical methods that break down the total variation in the data into pieces that correspond to different sources of variation. It aligns with the conceptual framework:</p>
<div class="math notranslate nohighlight">
\[\text{DATA = FIT + RESIDUAL.}\]</div>
<p>The total variation in the response <span class="math notranslate nohighlight">\(y\)</span> is expressed by the deviations <span class="math notranslate nohighlight">\(y_i - \bar{y}\)</span>. If these deviations were all zero, then all observations would be the same, implying no variation in <span class="math notranslate nohighlight">\(y\)</span>. When there is variation (i.e., <span class="math notranslate nohighlight">\(y_i\)</span> are not all equal to <span class="math notranslate nohighlight">\(\bar{y}\)</span>), the linear regression model identifies two sources for this variation:</p>
<ol class="arabic simple">
<li><p><strong>As the explanatory variable <span class="math notranslate nohighlight">\(x\)</span> changes</strong>, the mean response changes with it along the regression line. The fitted value <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> estimates the mean response for each <span class="math notranslate nohighlight">\(x_i\)</span>. The differences <span class="math notranslate nohighlight">\(\hat{y}_i - \bar{y}\)</span> reflect the variation in mean response due to differences in the <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
<li><p><strong>Individual observations vary</strong> about their subpopulation mean, captured by the residuals <span class="math notranslate nohighlight">\(y_i - \hat{y}_i\)</span> that record how much the actual observations scatter about the fitted line.</p></li>
</ol>
<p>Hence, each deviation <span class="math notranslate nohighlight">\(y_i - \bar{y}\)</span> can be split into:</p>
<div class="math notranslate nohighlight">
\[(y_i - \bar{y}) 
= (\hat{y}_i - \bar{y}) \;+\; (y_i - \hat{y}_i),\]</div>
<p>conveying the idea <strong>DATA = FIT + RESIDUAL</strong>.</p>
<p>Several Ways to Measure Variation:</p>
<p>We often measure variation by taking an average of squared deviations. By squaring each of the three deviations (data, fit, residual) and summing across <span class="math notranslate nohighlight">\(n\)</span> observations, we get:</p>
<div class="math notranslate nohighlight">
\[\sum (y_i - \bar{y})^2 \;=\; \sum (\hat{y}_i - \bar{y})^2 \;+\; \sum (y_i - \hat{y}_i)^2.\]</div>
<p>We rewrite it as:</p>
<div class="math notranslate nohighlight">
\[\text{SST} \;=\; \text{SSM} \;+\; \text{SSE},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \sum (y_i - \bar{y})^2, \quad
\text{SSM} = \sum (\hat{y}_i - \bar{y})^2, \quad
\text{SSE} = \sum (y_i - \hat{y}_i)^2.\]</div>
<ul class="simple">
<li><p><strong>SST</strong> (Total Sum of Squares) is the total variation in <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p><strong>SSM</strong> (Model Sum of Squares) measures the variation along the regression line (explained by <span class="math notranslate nohighlight">\(x\)</span>).</p></li>
<li><p><strong>SSE</strong> (Error Sum of Squares) is the residual or unexplained variation.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(b_1 = 0\)</span>, then <strong>SSM</strong> <span class="math notranslate nohighlight">\(= 0\)</span> and <strong>SST</strong> <span class="math notranslate nohighlight">\(=\)</span> <strong>SSE</strong>.</p>
<p>When <span class="math notranslate nohighlight">\(H_0\!: \beta_1 = 0\)</span> holds, there is no subpopulation structure and all <span class="math notranslate nohighlight">\(y_i\)</span> come from a single population with mean <span class="math notranslate nohighlight">\(\mu_y\)</span> (no linear dependence on <span class="math notranslate nohighlight">\(x\)</span>).</p>
<p>Sums of Squares, Degrees of Freedom, and Mean Squares:</p>
<div class="tip admonition">
<p class="admonition-title">SUMS OF SQUARES, DEGREES OF FREEDOM, AND MEAN SQUARES</p>
<p>Sums of squares represent the total variation present in the responses. We have:</p>
<div class="math notranslate nohighlight">
\[\text{SST = SSM + SSE}\]</div>
</div>
<p>with corresponding degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[\text{DFT = DFM + DFE}.\]</div>
<p>To calculate mean squares:</p>
<div class="math notranslate nohighlight">
\[\text{MS} = \frac{\text{sum of squares}}{\text{degrees of freedom}}.\]</div>
<p>From Section 2.4, recall that <span class="math notranslate nohighlight">\(r^2\)</span> is the fraction of the variation in <span class="math notranslate nohighlight">\(y\)</span> explained by the least-squares regression:</p>
<div class="math notranslate nohighlight">
\[r^2 
= \frac{\text{SSM}}{\text{SST}}
= \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2}.\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(r^2\)</span> is the <strong>proportion of explained variation</strong> in <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Mean Squares and MSE:</p>
<p>Recall that the sample variance for <span class="math notranslate nohighlight">\(y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[s_y^2 = \frac{\sum (y_i - \bar{y})^2}{n - 1},\]</div>
<p>which uses <strong>SST</strong> in the numerator and total degrees of freedom <span class="math notranslate nohighlight">\(\text{DFT} = n - 1\)</span>. Meanwhile, the degrees of freedom break down as:</p>
<div class="math notranslate nohighlight">
\[\text{DFT} = \text{DFM} + \text{DFE}.\]</div>
<p>For regression with one explanatory variable, <span class="math notranslate nohighlight">\(\text{DFM} = 1\)</span>, so <span class="math notranslate nohighlight">\(\text{DFE} = n - 2\)</span>. In general:</p>
<div class="math notranslate nohighlight">
\[\text{MS} 
= \frac{\text{sum of squares}}{\text{degrees of freedom}}
\quad\text{and}\quad
\text{MSE} = s^2 
= \frac{\sum (y_i - \hat{y}_i)^2}{n - 2}.\]</div>
</div>
<input id="sd-tab-item-9" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-9">
The ANOVA <span class="math notranslate nohighlight">\(F\)</span> Test</label><div class="sd-tab-content docutils">
<p>The null hypothesis <span class="math notranslate nohighlight">\(H_0\!: \beta_1 = 0\)</span> can be tested using the <strong><span class="math notranslate nohighlight">\(F\)</span> statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSM}}{\text{MSE}}.\]</div>
<p>If <span class="math notranslate nohighlight">\(H_0\)</span> is true, <span class="math notranslate nohighlight">\(F\)</span> follows an <span class="math notranslate nohighlight">\(F\)</span> distribution with 1 numerator df and <span class="math notranslate nohighlight">\(n - 2\)</span> denominator df. <span class="math notranslate nohighlight">\(F\)</span> distributions are right-skewed, indexed by <span class="math notranslate nohighlight">\(F(j, k)\)</span> where <span class="math notranslate nohighlight">\(j\)</span> is numerator df and <span class="math notranslate nohighlight">\(k\)</span> is denominator df. The peak is near 1.</p>
<p>Critical Values and Tables:</p>
<p><strong>Figure 10.14</strong> illustrates the density curve for <span class="math notranslate nohighlight">\(F(9, 10)\)</span>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/1203.png"><img alt="Figure 10.14" src="_images/1203.png" style="width: 80%;" /></a>
</figure>
<p>If software does not provide <span class="math notranslate nohighlight">\(P\)</span>-values, you can consult tables of critical <span class="math notranslate nohighlight">\(F\)</span> values (for example, <span class="math notranslate nohighlight">\(p = 0.100, 0.050, 0.025, 0.010, 0.001\)</span>). In simple linear regression, the numerator degrees of freedom = 1, and denominator df = <span class="math notranslate nohighlight">\(n - 2\)</span>.</p>
<p>ANOVA Table Structure:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id2">
<caption><span class="caption-number">Table 12.1 </span><span class="caption-text">Analysis of Variance</span><a class="headerlink" href="#id2" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 17.6%" />
<col style="width: 23.5%" />
<col style="width: 23.5%" />
<col style="width: 23.5%" />
<col style="width: 11.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>Degrees of Freedom</p></th>
<th class="head"><p>Sum of Squares</p></th>
<th class="head"><p>Mean Square</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(F\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Model</p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(\sum (\hat{y}_i - \bar{y})^2\)</span></p></td>
<td><p>SSM / DFM</p></td>
<td><p>MSM / MSE</p></td>
</tr>
<tr class="row-odd"><td><p>Error</p></td>
<td><p><span class="math notranslate nohighlight">\(n - 2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum (y_i - \hat{y}_i)^2\)</span></p></td>
<td><p>SSE / DFE</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n - 1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum (y_i - \bar{y})^2\)</span></p></td>
<td><p>SST / DFT</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/1204.png"><img alt="ANOVA Table" src="_images/1204.png" style="width: 80%;" /></a>
</figure>
<div class="tip admonition">
<p class="admonition-title">ANALYSIS OF VARIANCE <span class="math notranslate nohighlight">\(F\)</span> TEST</p>
<p>In regression, the hypotheses</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1 = 0 \quad \text{vs.}\quad H_a: \beta_1 \ne 0\]</div>
</div>
<p>are tested with the <strong>ANOVA <span class="math notranslate nohighlight">\(F\)</span> statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSM}}{\text{MSE}}.\]</div>
<p>The <span class="math notranslate nohighlight">\(P\)</span>-value is <span class="math notranslate nohighlight">\(P(F(1, n - 2) \ge F_{\text{obs}})\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(F\)</span> statistic tests the same null as the <span class="math notranslate nohighlight">\(t\)</span> statistic. In fact, <span class="math notranslate nohighlight">\(t^2 = F\)</span>. We often prefer the <span class="math notranslate nohighlight">\(t\)</span>-form in single-predictor regression for simplicity, especially for one-sided tests and confidence intervals.</p>
</div>
</div>
</section>
<section id="standard-errors">
<h2><span class="section-number">12.5. </span>Standard Errors<a class="headerlink" href="#standard-errors" title="Link to this heading">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-10" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-10">
Inference for Slope and Intercept</label><div class="sd-tab-content docutils">
<p>Confidence intervals and significance tests for the slope <span class="math notranslate nohighlight">\(\beta_1\)</span> and intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> of the population regression line use the estimates <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(b_0\)</span> and their standard errors. Some algebra and variance rules show that the standard deviation of <span class="math notranslate nohighlight">\(b_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\sigma_{b_1} = \frac{\sigma}{\sqrt{\sum (x_i - \bar{x})^2}}.\]</div>
<p>Similarly, the standard deviation of <span class="math notranslate nohighlight">\(b_0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\sigma_{b_0} 
= \sigma \,\sqrt{\,\frac{1}{n} \;+\; \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}\,}.\]</div>
<p>We estimate these by replacing <span class="math notranslate nohighlight">\(\sigma\)</span> with <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">STANDARD ERRORS FOR ESTIMATED REGRESSION COEFFICIENTS</p>
<p>The <strong>standard error of the slope</strong> <span class="math notranslate nohighlight">\(b_1\)</span> of the least-squares regression line is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{b_1} 
= \frac{s}{\sqrt{\sum (x_i - \bar{x})^2}}.\]</div>
<p>The <strong>standard error of the intercept</strong> <span class="math notranslate nohighlight">\(b_0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{b_0} 
= s \,\sqrt{\,\frac{1}{n} \;+\; \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}\,}.\]</div>
</div>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-11">
Confidence Intervals and Prediction Intervals</label><div class="sd-tab-content docutils">
<p>When we substitute a particular value <span class="math notranslate nohighlight">\(x^*\)</span> of the explanatory variable into the regression equation and obtain <span class="math notranslate nohighlight">\(\hat{y}\)</span>, there are two interpretations:</p>
<ol class="arabic simple">
<li><p><strong>Estimate the mean response</strong> <span class="math notranslate nohighlight">\(\mu_y\)</span> at <span class="math notranslate nohighlight">\(x = x^*\)</span>.</p></li>
<li><p><strong>Predict a future observation</strong> <span class="math notranslate nohighlight">\(y\)</span> at <span class="math notranslate nohighlight">\(x = x^*\)</span>.</p></li>
</ol>
<p>Prediction intervals are broader than confidence intervals for a mean. Both depend on <span class="math notranslate nohighlight">\(s\)</span>, the standard deviation about the fitted line.</p>
<div class="tip admonition">
<p class="admonition-title">STANDARD ERRORS FOR <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> AND <span class="math notranslate nohighlight">\(\hat{y}\)</span></p>
<p>The <strong>standard error of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span></strong> is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{\hat{\mu}}
= s \,\sqrt{\,\frac{1}{n} \;+\; \frac{(x^* - \bar{x})^2}{\sum (x_i - \bar{x})^2}\,}.\]</div>
<p>The <strong>standard error for predicting an individual response</strong> <span class="math notranslate nohighlight">\(\hat{y}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{\hat{y}}
= s \,\sqrt{\,1 \;+\; \frac{1}{n} \;+\; \frac{(x^* - \bar{x})^2}{\sum (x_i - \bar{x})^2}\,}.\]</div>
</div>
<p>The only difference is the extra “1” inside the square root for the prediction standard error, which accounts for the additional variation of individual responses around the mean.</p>
</div>
<input id="sd-tab-item-12" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-12">
Inference for Correlation</label><div class="sd-tab-content docutils">
<p>The correlation coefficient measures the strength and direction of <strong>linear</strong> association between two variables. We estimate the population correlation <span class="math notranslate nohighlight">\(\rho\)</span> using the sample correlation <span class="math notranslate nohighlight">\(r\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\rho = 0\)</span>, there is no linear association.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are <strong>jointly Normal</strong>, <span class="math notranslate nohighlight">\(\rho = 0\)</span> also implies <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are independent.</p></li>
</ul>
<p>Test for a Zero Population Correlation:</p>
<p>To test <span class="math notranslate nohighlight">\(H_0: \rho = 0\)</span>, compute the <span class="math notranslate nohighlight">\(t\)</span> statistic:</p>
<div class="math notranslate nohighlight">
\[t = \frac{r \,\sqrt{\,n - 2\,}}{\sqrt{\,1 - r^2\,}},\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the sample size. The <span class="math notranslate nohighlight">\(P\)</span>-value depends on the alternative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H_a\!: \rho &gt; 0 &amp; \quad\Rightarrow\quad P(T \ge t), \\
H_a\!: \rho &lt; 0 &amp; \quad\Rightarrow\quad P(T \le t), \\
H_a\!: \rho \ne 0 &amp; \quad\Rightarrow\quad 2\,P(T \ge |t|).
\end{aligned}\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">TEST FOR ZERO CORRELATION</p>
<p>If <span class="math notranslate nohighlight">\(T\)</span> has a <span class="math notranslate nohighlight">\(t(n - 2)\)</span> distribution, then the <span class="math notranslate nohighlight">\(P\)</span>-value is found by comparing <span class="math notranslate nohighlight">\(|t|\)</span> to that distribution.</p>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/1205.png"><img alt="Inference for Correlation" src="_images/1205.png" style="width: 80%;" /></a>
</figure>
</div>
</div>
</section>
<section id="more-details-on-standard-errors-of-mean-response-and-prediction">
<h2><span class="section-number">12.6. </span>More details on standard errors of mean response and prediction<a class="headerlink" href="#more-details-on-standard-errors-of-mean-response-and-prediction" title="Link to this heading">#</a></h2>
<div class="math notranslate nohighlight" id="equation-eq-linear-model">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-eq-linear-model" title="Link to this equation">#</a></span>\[y_i = \beta_0 + \beta_1 x_i + \varepsilon_i 
\quad\text{where}\quad 
\varepsilon_i \sim \mathcal{N}(0,\sigma^2).\]</div>
<p>Under OLS estimates, <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> have the following properties in <strong>simple</strong> linear regression (one predictor):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \mathrm{Var}(\hat{\beta}_1) = \dfrac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2},\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \mathrm{Var}(\hat{\beta}_0) = \sigma^2 \left[\dfrac{1}{n} + \dfrac{\overline{x}^2}{\sum_{i=1}^n (x_i - \overline{x})^2}\right],\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \mathrm{Cov}(\hat{\beta}_0,\hat{\beta}_1) = -\,\overline{x}\,\dfrac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2}.\)</span></p></li>
</ul>
<p>For any new point <span class="math notranslate nohighlight">\(x_0\)</span>, the fitted <strong>mean response</strong> is</p>
<div class="math notranslate nohighlight" id="equation-eq-mean-response">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-eq-mean-response" title="Link to this equation">#</a></span>\[\hat{\mu}_{y_0} \;=\; \hat{\beta}_0 \;+\; \hat{\beta}_1 \, x_0.\]</div>
<section id="deriving-mathrm-var-hat-mu-y-0">
<h3><span class="section-number">12.6.1. </span>Deriving <span class="math notranslate nohighlight">\( \mathrm{Var}(\hat{\mu}_{y_0}) \)</span><a class="headerlink" href="#deriving-mathrm-var-hat-mu-y-0" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight" id="equation-eq-var-mean-derivation">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-eq-var-mean-derivation" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\mathrm{Var}(\hat{\mu}_{y_0})
&amp;= \mathrm{Var}(\hat{\beta}_0 + x_0\,\hat{\beta}_1) \\
&amp;= \mathrm{Var}(\hat{\beta}_0)
 + x_0^2\,\mathrm{Var}(\hat{\beta}_1)
 + 2\,x_0\,\mathrm{Cov}(\hat{\beta}_0,\hat{\beta}_1).
\end{aligned}\end{split}\]</div>
<p>Plugging in the standard results from above:</p>
<div class="math notranslate nohighlight" id="equation-eq-var-mean-expand">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-eq-var-mean-expand" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\mathrm{Var}(\hat{\mu}_{y_0})
&amp;= \sigma^2
\left[\frac{1}{n} + \frac{\overline{x}^2}{\sum (x_i - \overline{x})^2}\right]
\;+\;
x_0^2\; \sigma^2\!\biggl/\!\sum (x_i - \overline{x})^2
\;+\;
2\,x_0 \Bigl(-\,\overline{x}\,\dfrac{\sigma^2}{\sum (x_i - \overline{x})^2}\Bigr) \\
&amp;= \sigma^2 \left[
  \frac{1}{n}
  \;+\;
  \frac{(x_0 - \overline{x})^2}{\sum (x_i - \overline{x})^2}
\right].
\end{aligned}\end{split}\]</div>
<p>Typically, we do <strong>not</strong> know <span class="math notranslate nohighlight">\(\sigma^2\)</span>; we replace it by <span class="math notranslate nohighlight">\(\mathrm{MSE}\)</span>, the mean squared error estimate from the regression. Thus,</p>
<div class="math notranslate nohighlight" id="equation-eq-var-mean-response">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-eq-var-mean-response" title="Link to this equation">#</a></span>\[\mathrm{Var}(\hat{\mu}_{y_0})
\;=\;
\mathrm{MSE}
\left[
\frac{1}{n}
\;+\;
\frac{(x_0 - \overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}
\right].\]</div>
<p>Hence, the <strong>standard error</strong> of the estimated mean response at <span class="math notranslate nohighlight">\(x_0\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-se-mean-response">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-eq-se-mean-response" title="Link to this equation">#</a></span>\[\mathrm{SE}_{\hat{\mu}_y}
\;=\;
\sqrt{
   \mathrm{Var}(\hat{\mu}_{y_0})
}
\;=\;
\sqrt{
   \mathrm{MSE}
   \left[
      \frac{1}{n}
      \;+\;
      \frac{(x_0 - \overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}
   \right]
}.\]</div>
</section>
<section id="future-observation-prediction-interval">
<h3><span class="section-number">12.6.2. </span>Future Observation (Prediction Interval)<a class="headerlink" href="#future-observation-prediction-interval" title="Link to this heading">#</a></h3>
<p>When predicting a <strong>future</strong> <span class="math notranslate nohighlight">\(y\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span> — call it <span class="math notranslate nohighlight">\(\hat{y}_0\)</span> — we add in the irreducible noise <span class="math notranslate nohighlight">\(\sigma^2\)</span> from the new observation itself. In other words:</p>
<div class="math notranslate nohighlight" id="equation-eq-var-prediction">
<span class="eqno">(12.7)<a class="headerlink" href="#equation-eq-var-prediction" title="Link to this equation">#</a></span>\[\mathrm{Var}(\hat{y}_0)
\;=\;
\mathrm{MSE}
\Bigl[
  1
  \;+\;
  \frac{1}{n}
  \;+\;
  \frac{(x_0 - \overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}
\Bigr].\]</div>
<p>Thus the <strong>standard error</strong> for a future observation (used in a prediction interval) is</p>
<div class="math notranslate nohighlight" id="equation-eq-se-prediction">
<span class="eqno">(12.8)<a class="headerlink" href="#equation-eq-se-prediction" title="Link to this equation">#</a></span>\[\mathrm{SE}_{\hat{y}}
\;=\;
\sqrt{\mathrm{Var}(\hat{y}_0)}
\;=\;
\sqrt{
   \mathrm{MSE}
   \left[
      1
      \;+\;
      \frac{1}{n}
      \;+\;
      \frac{(x_0 - \overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}
   \right]
}.\]</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnote01" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>In frequentist inference, we consider the parameters as fixed (but unknown) quantities, and the data as random due to sampling variability. In Bayesian inference, we treat the parameters as random variables with their own distributions (priors), and once the data is observed, it is considered fixed while we update our beliefs about the parameters through the posterior distribution.</p>
</aside>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter11.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Chapter 2: Least Squares Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter13.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Chapter 11: Multiple Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">12.1. Simple Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-parameters">12.2. Estimating the regression parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-significant-tests">12.3. Confidence intervals and significant tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-detail-about-simple-linear-regression">12.4. More detail about simple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-errors">12.5. Standard Errors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details-on-standard-errors-of-mean-response-and-prediction">12.6. More details on standard errors of mean response and prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-mathrm-var-hat-mu-y-0">12.6.1. Deriving <span class="math notranslate nohighlight">\( \mathrm{Var}(\hat{\mu}_{y_0}) \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-observation-prediction-interval">12.6.2. Future Observation (Prediction Interval)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank (Chenzhong) Wu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>