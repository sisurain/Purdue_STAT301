
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7. Chapter 7: Inference for Means &#8212; STAT301@Purdue</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter7';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="6. Chapter 6: Confidence Intervals and Significance Tests" href="chapter6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro0.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/statlogo.png" class="logo__image only-light" alt="STAT301@Purdue - Home"/>
    <img src="_static/statlogo.png" class="logo__image only-dark pst-js-only" alt="STAT301@Purdue - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro0.html">
                    STAT 301 Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">At the beginning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">1. At the Beginning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Working with data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">2. Some Important Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">3. Chapter 3: Producing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">4. Chapter 1: Looking at Data – Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Making statistical inference</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">5. Chapter 5: Sampling Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">6. Chapter 6: Confidence Intervals and Significance Tests</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. Chapter 7: Inference for Means</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter7.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter7.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 7: Inference for Means</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-for-the-mean-of-a-population">7.1. Inference for the Mean of a Population</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-two-means">7.2. Comparing Two Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-size-calculations">7.3. Sample Size Calculations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiasedness-and-consistency-of-the-sample-standard-deviation">7.4. Unbiasedness and Consistency of the Sample Standard Deviation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-7-inference-for-means">
<h1><span class="section-number">7. </span>Chapter 7: Inference for Means<a class="headerlink" href="#chapter-7-inference-for-means" title="Link to this heading">#</a></h1>
<p>In the last chapter, we encountered the population parameter <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong> when calculating the <strong>margin of error</strong> for confidence intervals and computing the <strong><span class="math notranslate nohighlight">\(z\)</span> test statistic</strong> for hypothesis testing. However, in practice, this <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong> is usually <strong>unknown</strong>.</p>
<p><strong>Addressing the Unknown <span class="math notranslate nohighlight">\(\sigma\)</span>:</strong></p>
<ul>
<li><p>In this chapter, we address this issue by making a more <strong>realistic assumption</strong>-that we do <strong>not</strong> know the value of <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong>-and explore how we can still conduct inference under two key settings:</p>
<ol class="arabic simple">
<li><p><strong>Single-Sample Inference:</strong></p></li>
</ol>
<ul class="simple">
<li><p>We have <strong>one sample</strong> and are interested in estimating the <strong>population mean</strong> <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>This scenario mirrors what we saw in the previous chapter but now incorporates the <strong>unknown <span class="math notranslate nohighlight">\(\sigma\)</span></strong>.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Two-Sample Inference:</strong></p></li>
</ol>
<ul class="simple">
<li><p>We have <strong>two independent samples</strong>, meaning we are dealing with <strong>two populations</strong>.</p></li>
<li><p>Our goal is to analyze the <strong>difference between the two population means</strong>, denoted as <strong><span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span></strong>.</p></li>
</ul>
</li>
</ul>
<p><strong>Real-World Applications:</strong></p>
<ul class="simple">
<li><p>Many real-world problems fit into one of these two frameworks. Whether comparing <strong>before-and-after treatment effects, different experimental conditions, or population differences</strong>, these two settings provide a foundation for statistical inference when the <strong>population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown</strong>.</p></li>
</ul>
<section id="inference-for-the-mean-of-a-population">
<h2><span class="section-number">7.1. </span>Inference for the Mean of a Population<a class="headerlink" href="#inference-for-the-mean-of-a-population" title="Link to this heading">#</a></h2>
<p>If we do <strong>not</strong> know the population standard deviation <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong>, we need to find an <strong>estimator</strong> to approximate it-just as we use the <strong>sample mean</strong> <strong><span class="math notranslate nohighlight">\(\bar{x}\)</span></strong> to estimate the <strong>population mean</strong> <strong><span class="math notranslate nohighlight">\(\mu\)</span></strong>. Naturally, a reasonable choice is the <strong>sample standard deviation</strong> <strong><span class="math notranslate nohighlight">\(s\)</span></strong> as an estimator for the <strong>population standard deviation</strong> <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong>.</p>
<p><strong>Why Use <span class="math notranslate nohighlight">\(s\)</span> Instead of <span class="math notranslate nohighlight">\(\sigma\)</span>?</strong></p>
<ul class="simple">
<li><p>The <strong>sample standard deviation</strong> <span class="math notranslate nohighlight">\(s\)</span> is a <strong>good estimator</strong> of the population counterpart <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>We can show that <strong><span class="math notranslate nohighlight">\(s\)</span> is an unbiased estimator</strong> of <span class="math notranslate nohighlight">\(\sigma\)</span>, meaning its expected value equals <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>It is also a <strong>consistent estimator</strong>, meaning that as the <strong>sample size <span class="math notranslate nohighlight">\(n\)</span> increases</strong>, <span class="math notranslate nohighlight">\(s\)</span> gets closer to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
<p><strong>Standard Error:</strong></p>
<ul>
<li><p>Since we do not know <span class="math notranslate nohighlight">\(\sigma\)</span>, we also need to estimate the <strong>standard deviation of the sample mean</strong>, which is:</p>
<div class="math notranslate nohighlight">
\[\frac{\sigma}{\sqrt{n}}.\]</div>
</li>
<li><p>By replacing <span class="math notranslate nohighlight">\(\sigma\)</span> with its estimator <span class="math notranslate nohighlight">\(s\)</span>, we obtain the standard error (SE) of our sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{\bar{x}} = \frac{s}{\sqrt{n}}.\]</div>
</li>
</ul>
<p>The standard error quantifies the expected variability of the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> across different samples and plays a crucial role in confidence intervals and hypothesis testing.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Standard Error of the Sample Mean</label><div class="sd-tab-content docutils">
<p><strong>1. Standard Error of the Sample Mean</strong></p>
<ul class="simple">
<li><p>We have a sample of size <span class="math notranslate nohighlight">\(n\)</span> from a normally distributed population with true mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>The sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> has <em>population</em> standard deviation <span class="math notranslate nohighlight">\(\frac{\sigma}{\sqrt{n}}\)</span>.</p></li>
<li><p><strong>Unknown</strong> <span class="math notranslate nohighlight">\(\sigma\)</span>: If <span class="math notranslate nohighlight">\(\sigma\)</span> is not known, we estimate it with the <em>sample</em> standard deviation <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>Therefore, the estimated standard deviation of <span class="math notranslate nohighlight">\(\bar{x}\)</span> is: <span class="math notranslate nohighlight">\(\text{SE}_{\bar{x}} = \frac{s}{\sqrt{n}}\)</span>.</p></li>
<li><p>This quantity is called the <strong>standard error</strong> of the sample mean. It represents an <em>estimate</em> of how much <span class="math notranslate nohighlight">\(\bar{x}\)</span> varies from sample to sample.</p></li>
</ul>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
<span class="math notranslate nohighlight">\(z\)</span> vs. <span class="math notranslate nohighlight">\(t\)</span> Statistics</label><div class="sd-tab-content docutils">
<p><strong>2. <span class="math notranslate nohighlight">\(z\)</span> vs. <span class="math notranslate nohighlight">\(t\)</span> Statistics</strong></p>
<ul>
<li><p>When <span class="math notranslate nohighlight">\(\sigma\)</span> <em>is</em> known, we can use the <strong>one-sample <span class="math notranslate nohighlight">\(z\)</span> statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}\]</div>
<p>which follows a standard normal distribution <span class="math notranslate nohighlight">\(N(0,1)\)</span>.</p>
</li>
<li><p><strong>However</strong>, when <span class="math notranslate nohighlight">\(\sigma\)</span> is <em>not</em> known and is replaced by <span class="math notranslate nohighlight">\(s\)</span>, the statistic</p>
<div class="math notranslate nohighlight">
\[t = \frac{\bar{x} - \mu}{s / \sqrt{n}}\]</div>
<p>does <em>not</em> follow a standard normal distribution. Instead, it follows a <strong><span class="math notranslate nohighlight">\(t\)</span>-distribution</strong> with <span class="math notranslate nohighlight">\(n - 1\)</span> degrees of freedom.</p>
</li>
</ul>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
The <span class="math notranslate nohighlight">\(t\)</span>-Distribution</label><div class="sd-tab-content docutils">
<p><strong>3. The <span class="math notranslate nohighlight">\(t\)</span>-Distribution</strong></p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(t\)</span>-distribution arises naturally when we estimate <span class="math notranslate nohighlight">\(\sigma\)</span> by <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>It looks similar to the standard normal distribution but has thicker tails, especially for smaller sample sizes.</p></li>
<li><p>As <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>, the <span class="math notranslate nohighlight">\(t\)</span>-distribution converges to the standard normal.</p></li>
<li><p>In practice, the <span class="math notranslate nohighlight">\(t\)</span>-distribution allows us to perform <em>inference on the mean</em> when the population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown, using <span class="math notranslate nohighlight">\(s\)</span> as an estimator.</p></li>
</ul>
</div>
</div>
<p>Another setting where the one-sample <em>t</em> procedure is often used is in a <strong>matched-pairs</strong> design.</p>
<p><strong>In a matched-pairs setup:</strong></p>
<ul class="simple">
<li><p>Each pair of observations is combined into a <em>single</em> difference value, yielding a single set of differences (one per pair).</p></li>
<li><p>Because these differences form one sample, the standard one-sample <em>t</em> test applies naturally to determine whether the <em>average</em> difference is zero.</p></li>
</ul>
<p>The logic of a one-sample test remains the same-now we simply use within-pair differences rather than two separate sets of measurements.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Example 7.7 Scenario</label><div class="sd-tab-content docutils">
<p><strong>Scenario</strong></p>
<ul class="simple">
<li><p><strong>Study Goal</strong>: Compare measurements of machine parts with and without a certain “option” enabled in the measuring software.</p></li>
<li><p><strong>Data</strong>: Each part gives two measurements: <em>Option On</em> and <em>Option Off</em>. The difference <span class="math notranslate nohighlight">\(d_i = (\text{On}_i - \text{Off}_i)\)</span> is recorded for each part <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><strong>Sample Size</strong>: <span class="math notranslate nohighlight">\(n = 51\)</span> parts, so we have 51 paired differences <span class="math notranslate nohighlight">\(d_1, d_2, \dots, d_{51}\)</span>.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/0701.png"><img alt="table 7.2" src="_images/0701.png" style="width: 70%;" /></a>
</figure>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
Hypotheses</label><div class="sd-tab-content docutils">
<p><strong>Hypotheses</strong></p>
<p>We test:</p>
<div class="math notranslate nohighlight">
\[H_0: \mu = 0 
\quad\text{vs.}\quad
H_a: \mu \neq 0,\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the <em>population mean</em> of the difference in measurements (<em>On</em> minus <em>Off</em>).</p>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-5">
<span class="math notranslate nohighlight">\(t\)</span> Test</label><div class="sd-tab-content docutils">
<p><strong>Key Summary Statistics</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{d} = 0.0504\)</span> (mean of the 51 differences)</p></li>
<li><p><span class="math notranslate nohighlight">\(s = 0.6943\)</span> (sample standard deviation of the differences)</p></li>
</ul>
<p><strong>One-Sample <span class="math notranslate nohighlight">\(t\)</span> Statistic</strong></p>
<div class="math notranslate nohighlight">
\[t = \frac{\bar{d} - 0}{s / \sqrt{n}}
= \frac{0.0504}{0.6943 / \sqrt{51}}
\approx 0.52.\]</div>
<p>This statistic follows a <span class="math notranslate nohighlight">\(t\)</span> distribution with <span class="math notranslate nohighlight">\(n - 1 = 50\)</span> degrees of freedom under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
</div>
<input id="sd-tab-item-6" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-6">
Interpretation</label><div class="sd-tab-content docutils">
<p><strong>Interpretation of Results</strong></p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(t = 0.52\)</span></strong>, <span class="math notranslate nohighlight">\(df = 50\)</span>.</p></li>
<li><p>The <em>p</em>-value (two-tailed) is greater than 0.50 (software reports approximately 0.6054).</p></li>
<li><p><strong>Conclusion</strong>: There is no statistically significant evidence (<span class="math notranslate nohighlight">\(p \approx 0.61\)</span>) to conclude that the measuring option changes the average measurement.</p></li>
</ul>
<p>When reporting results, it is common to say:</p>
<blockquote>
<div><p>“The difference in measurements was not statistically significant (<span class="math notranslate nohighlight">\(t = 0.52\)</span>, <span class="math notranslate nohighlight">\(df = 50\)</span>, <span class="math notranslate nohighlight">\(p = 0.61\)</span>).”</p>
</div></blockquote>
</div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><strong>A Lack of Statistical Significance <span class="math notranslate nohighlight">\(\neq\)</span> Proof of <span class="math notranslate nohighlight">\(H_0\)</span></strong></p>
<ul class="simple">
<li><p>Failing to reject <span class="math notranslate nohighlight">\(H_0\)</span> does <strong>not</strong> mean we have proven <span class="math notranslate nohighlight">\(H_0\)</span> true; it often means that our study lacked sufficient evidence (or power) to show a difference.</p></li>
<li><p>If we could “prove” a null hypothesis by simply getting a non-significant result, we could always design weak experiments to achieve that outcome. Clearly, this is not sound science.</p></li>
</ul>
</div>
<p>Here, we can <strong>shift our question</strong> slightly to:</p>
<blockquote>
<div><p><strong>“Is any difference too <em>small</em> to be important?”</strong></p>
</div></blockquote>
<p>By <strong>pre-specifying</strong> an acceptable difference <strong><span class="math notranslate nohighlight">\(\delta\)</span></strong>, we conduct <strong>Equivalence Testing</strong>, which focuses on <strong>practical</strong> rather than purely <strong>statistical</strong> significance.</p>
<p><strong>Why Standard Hypothesis Testing Is Not Enough:</strong></p>
<ul class="simple">
<li><p>A <strong>nonsignificant</strong> result in a standard hypothesis test does <strong>not</strong> necessarily mean two groups are equivalent.</p></li>
<li><p>Instead of simply failing to reject <span class="math notranslate nohighlight">\(H_0\)</span>, we need to <strong>demonstrate with high confidence</strong> that the difference is within an agreed-upon small range.</p></li>
<li><p>The traditional <strong>“test vs. zero”</strong> framework does not establish <strong>equivalence</strong>-it only tests for a lack of sufficient evidence to reject <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
</ul>
<p><strong>Equivalence Testing Logic:</strong></p>
<ul class="simple">
<li><p>We <strong>define a small “practical” difference</strong> (threshold) <span class="math notranslate nohighlight">\(\delta\)</span> below which two means are deemed “close enough” to be considered equivalent.</p></li>
<li><p>We then ask if our data strongly indicate that any true difference <em>cannot</em> exceed <span class="math notranslate nohighlight">\(\delta\)</span> in magnitude.</p></li>
<li><p>Concretely, we build a confidence interval around our estimated difference.</p>
<ul>
<li><p>If <em>the entire CI</em> fits within <span class="math notranslate nohighlight">\((-\delta,\ \delta)\)</span>, we declare the two means “equivalent.”</p></li>
<li><p>If part of the CI lies outside <span class="math notranslate nohighlight">\((-\delta,\ \delta)\)</span>, we lack evidence to conclude equivalence.</p></li>
</ul>
</li>
</ul>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-7" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-7">
Equivalence Testing</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p><strong>Motivation</strong>: Sometimes we want to show that two treatments or measurements are <em>practically the same</em> (i.e., any difference is too small to matter). A traditional hypothesis test merely tells us whether a difference is <em>detectable</em>, not whether it is <em>small</em>.</p></li>
<li><p><strong>Approach</strong>:</p>
<ol class="arabic simple">
<li><p>Specify an <em>equivalence range</em> around the null value, say <span class="math notranslate nohighlight">\(\mu_0 \pm \delta\)</span>. Any difference within <span class="math notranslate nohighlight">\(\pm \delta\)</span> is deemed “unimportant.”</p></li>
<li><p>Construct a confidence interval (CI) for the parameter of interest (often a mean difference).</p></li>
<li><p>If the entire CI lies <em>inside</em> <span class="math notranslate nohighlight">\(\mu_0 \pm \delta\)</span>, we <em>conclude equivalence</em>. Otherwise, we lack evidence to declare them equivalent.</p></li>
</ol>
</li>
</ul>
<blockquote>
<div><p><strong>Key Point</strong>: Equivalence testing focuses on whether the true mean difference is <em>small enough</em> to be negligible, rather than simply testing if it differs from zero.</p>
</div></blockquote>
</div>
<input id="sd-tab-item-8" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-8">
Example 7.8</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p><strong>Scenario</strong>: Researchers consider a mean difference less than 0.20 micron “not important.”</p></li>
<li><p><strong>Data</strong>: We have a sample of size <span class="math notranslate nohighlight">\(n=51\)</span>, yielding <span class="math notranslate nohighlight">\(\bar{x} = 0.0504\)</span>, and <span class="math notranslate nohighlight">\(\text{SE}_{\bar{x}}=0.0972\)</span>.</p></li>
<li><p><strong>90% CI</strong>: <span class="math notranslate nohighlight">\(\bar{x} \pm t^*\,\text{SE}_{\bar{x}} 
= 0.0504 \pm (1.676)(0.0972) 
= (-0.112,\ 0.2133).\)</span></p></li>
</ul>
<p>Since <span class="math notranslate nohighlight">\(-0.112\)</span> and <span class="math notranslate nohighlight">\(0.2133\)</span> are <em>not</em> fully contained in the “equivalence region” <span class="math notranslate nohighlight">\([-0.20,\ 0.20]\)</span>, we <strong>cannot</strong> conclude equivalence at the 5% significance level. However, the mean difference is quite close to zero, so a larger sample might shrink the CI and yield a different conclusion.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/0702.png"><img alt="figure 7.7" src="_images/0702.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Finally, we should discuss the <strong>robustness</strong> of <em>t</em> procedures and possible <strong>alternatives</strong>, especially when dealing with <strong>non-normal datasets</strong>.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-9" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-9">
Why <em>t</em> Procedures Are Robust</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p><strong>Central Limit Theorem</strong>: For large samples, the distribution of the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> is approximately Normal, regardless of the population distribution.</p></li>
<li><p><strong>Law of Large Numbers</strong>: As <span class="math notranslate nohighlight">\(n\)</span> grows, the sample standard deviation <span class="math notranslate nohighlight">\(s\)</span> becomes a reliable estimate of the true <span class="math notranslate nohighlight">\(\sigma\)</span>, making <em>t</em> tests generally reliable even if the population is not perfectly Normal.</p></li>
</ul>
</div>
<input id="sd-tab-item-10" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-10">
Practical Guidelines</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p><strong>Sample size &lt; 15</strong>: Use <em>t</em> procedures only if the data are reasonably close to Normal (no strong skew/outliers).</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(15 \le n &lt; 40\)</span></strong>: <em>t</em> procedures are typically fine unless you see strong skew or obvious outliers.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(n \ge 40\)</span></strong>: Even clearly skewed data can often be handled with <em>t</em> procedures.</p></li>
</ul>
<p>These are rules of thumb for a single mean. The main concern is the presence of serious non-Normality or outliers in small samples.</p>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-11">
Inference for Non-Normal Populations</label><div class="sd-tab-content docutils">
<p><strong>3. Inference for Non-Normal Populations</strong></p>
<p>When data are clearly non-Normal and <span class="math notranslate nohighlight">\(n\)</span> is not large enough to rely on the robustness of <em>t</em> tests, you have several options:</p>
<ul class="simple">
<li><p><strong>Use a Different Parametric Model</strong></p>
<ul>
<li><p>Some distributions (e.g., Poisson, exponential, etc.) have specialized methods if you can justify them.</p></li>
</ul>
</li>
<li><p><strong>Transform the Data</strong></p>
<ul>
<li><p><em>Skewness</em> often improves under a transformation like <span class="math notranslate nohighlight">\(\log(x)\)</span>.</p></li>
<li><p>Then apply the <em>t</em> procedure to the <em>transformed</em> values. Results are more accurate if the transformed data are closer to Normal.</p></li>
<li><p><em>Con</em>: Interpreting confidence intervals in the original scale can be trickier.</p></li>
</ul>
</li>
<li><p><strong>Distribution-Free (Nonparametric) Procedures</strong></p>
<ul>
<li><p>These methods do <em>not</em> assume the population follows a Normal distribution.</p></li>
<li><p><em>Pro</em>: More general.</p></li>
<li><p><em>Con</em>: Often <em>less powerful</em> if the data were in fact close to Normal. Also, many such methods focus on the <em>median</em> rather than the <em>mean</em>.</p></li>
</ul>
</li>
</ul>
</div>
</div>
</section>
<section id="comparing-two-means">
<h2><span class="section-number">7.2. </span>Comparing Two Means<a class="headerlink" href="#comparing-two-means" title="Link to this heading">#</a></h2>
<p>Now, let’s shift our focus to <strong>two samples from two distinct populations</strong>. This scenario arises frequently in many real-world applications where we wish to compare <strong>two population means</strong>.</p>
<p>Fortunately, under <strong>mild conditions</strong>, the difference between the two sample means, <strong><span class="math notranslate nohighlight">\(\bar{X}_1 - \bar{X}_2\)</span></strong>, is <strong>approximately normal</strong>. This allows us to extend the <strong>statistical procedures</strong> we learned for the <strong>one-sample case</strong> to this new <strong>two-sample setting</strong>.</p>
<p><strong>Key Insights:</strong></p>
<ul class="simple">
<li><p>Just as in the one-sample case, we have both <strong><span class="math notranslate nohighlight">\(z\)</span> statistics</strong> and <strong><span class="math notranslate nohighlight">\(t\)</span> statistics</strong> for two-sample inference.</p></li>
<li><p>When the population standard deviations <strong><span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span></strong> are <strong>unknown and unequal</strong>, we use the <strong>standard two-sample <span class="math notranslate nohighlight">\(t\)</span> procedure</strong>.</p></li>
<li><p>However, if the two populations are assumed to have <strong>equal standard deviations</strong> (<strong><span class="math notranslate nohighlight">\(\sigma_1 = \sigma_2\)</span></strong>), we can <strong>pool the variance</strong> to obtain a <strong>more efficient</strong> procedure, known as the <strong>pooled two-sample <span class="math notranslate nohighlight">\(t\)</span> test</strong>.</p></li>
</ul>
<p>By leveraging these different approaches, we can <strong>increase the accuracy and efficiency</strong> of our statistical inference when comparing two population means.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Goals and Setup</strong></p>
<ul class="simple">
<li><p><strong>Objective</strong>: Compare the means of a response variable in two different groups (populations).</p></li>
<li><p><strong>Examples</strong>:</p>
<ul>
<li><p>Comparing college students’ impressions from Wisconsin vs. Indiana.</p></li>
<li><p>Testing two different diets and measuring their impact on blood pressure.</p></li>
<li><p>Evaluating two incentive plans for debit card usage.</p></li>
</ul>
</li>
<li><p><strong>Key Conditions</strong>:</p>
<ul>
<li><p>Each group is viewed as a <em>distinct</em> population.</p></li>
<li><p>We gather <em>independent</em> SRSs: one from each population.</p></li>
<li><p>The responses in group 1 are independent from those in group 2.</p></li>
</ul>
</li>
</ul>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" for="sd-tab-item-12">
Two-Sample <em>z</em> Statistic</label><div class="sd-tab-content docutils">
<p><strong>Notation</strong></p>
<ul class="simple">
<li><p><strong>Population 1</strong>: Mean <span class="math notranslate nohighlight">\(\mu_1\)</span>, Standard Deviation <span class="math notranslate nohighlight">\(\sigma_1\)</span>, Sample Size <span class="math notranslate nohighlight">\(n_1\)</span>, Sample Mean <span class="math notranslate nohighlight">\(\bar{x}_1\)</span>, Sample SD <span class="math notranslate nohighlight">\(s_1\)</span>.</p></li>
<li><p><strong>Population 2</strong>: Mean <span class="math notranslate nohighlight">\(\mu_2\)</span>, Standard Deviation <span class="math notranslate nohighlight">\(\sigma_2\)</span>, Sample Size <span class="math notranslate nohighlight">\(n_2\)</span>, Sample Mean <span class="math notranslate nohighlight">\(\bar{x}_2\)</span>, Sample SD <span class="math notranslate nohighlight">\(s_2\)</span>.</p></li>
</ul>
<p>We often compare <span class="math notranslate nohighlight">\(\mu_1\)</span> and <span class="math notranslate nohighlight">\(\mu_2\)</span> by examining the difference <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>. In practice, we estimate this using <span class="math notranslate nohighlight">\(\bar{x}_1 - \bar{x}_2\)</span>.</p>
<p><strong>Two-Sample <em>z</em> Statistic (Population SDs Known)</strong></p>
<p>If <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> are <em>known</em>:</p>
<ol class="arabic simple">
<li><p>The <strong>sampling distribution</strong> of <span class="math notranslate nohighlight">\((\bar{x}_1 - \bar{x}_2)\)</span> is Normal if both populations are Normal (or <span class="math notranslate nohighlight">\(n_1, n_2\)</span> are large enough by the Central Limit Theorem).</p></li>
<li><p><strong>Mean</strong> of <span class="math notranslate nohighlight">\((\bar{x}_1 - \bar{x}_2)\)</span> is <span class="math notranslate nohighlight">\((\mu_1 - \mu_2)\)</span>.</p></li>
<li><p><strong>Variance</strong> of <span class="math notranslate nohighlight">\((\bar{x}_1 - \bar{x}_2)\)</span> is <span class="math notranslate nohighlight">\(\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}\)</span>, assuming independence between the two samples.</p></li>
</ol>
<p>The <strong>two-sample <span class="math notranslate nohighlight">\(z\)</span> statistic</strong> is:</p>
<div class="math notranslate nohighlight">
\[z \;=\; \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}.\]</div>
</div>
<input id="sd-tab-item-13" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" for="sd-tab-item-13">
Two-Sample <em>t</em> Procedures</label><div class="sd-tab-content docutils">
<p><strong>When Population SDs Are Unknown: Two-Sample <em>t</em> Procedures</strong></p>
<ul class="simple">
<li><p>Typically, <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> are unknown in real-world settings.</p></li>
<li><p>We replace <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> by their sample estimates <span class="math notranslate nohighlight">\(s_1\)</span> and <span class="math notranslate nohighlight">\(s_2\)</span>.</p></li>
<li><p>The resulting test or confidence interval is the <strong>two-sample <em>t</em></strong> procedure.</p></li>
<li><p>As with the one-sample case, if the sample sizes are reasonably large (or if each group is approximately Normal), these <em>t</em> procedures are robust to moderate deviations from Normality.</p></li>
</ul>
<p>We’ll derive the exact form of the <strong>two-sample <em>t</em> statistic</strong>, its degrees of freedom, and how to construct confidence intervals for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>.</p>
<p><strong>1. When Do We Use Two-Sample <em>t</em> Procedures?</strong></p>
<ul class="simple">
<li><p><strong>Unknown <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span></strong>: In real-world settings, population standard deviations are typically unknown.</p></li>
<li><p><strong>Two Independent SRSs</strong>: We draw samples of sizes <span class="math notranslate nohighlight">\(n_1\)</span> and <span class="math notranslate nohighlight">\(n_2\)</span> from two populations with means <span class="math notranslate nohighlight">\(\mu_1\)</span> and <span class="math notranslate nohighlight">\(\mu_2\)</span>, respectively.</p></li>
<li><p><strong>Comparison of Means</strong>: We want to infer about <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>, either via hypothesis testing (e.g., <span class="math notranslate nohighlight">\(H_0: \mu_1 = \mu_2\)</span>) or a confidence interval (CI) for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>.</p></li>
</ul>
<p>By replacing <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> with their sample estimates (<span class="math notranslate nohighlight">\(s_1\)</span> and <span class="math notranslate nohighlight">\(s_2\)</span>), we obtain a <em>t</em>-based procedure instead of the (rarely used) two-sample <span class="math notranslate nohighlight">\(z\)</span> procedure.</p>
<p><strong>2. The Two-Sample <em>t</em> Statistic</strong></p>
<p>When <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> are unknown, the <strong>two-sample <em>t</em> statistic</strong> is</p>
<div class="math notranslate nohighlight">
\[t \;=\; \frac{\bigl(\bar{x}_1 - \bar{x}_2\bigr) - \bigl(\mu_1 - \mu_2\bigr)}{\sqrt{\frac{s_1^2}{n_1} \;+\; \frac{s_2^2}{n_2}}}.\]</div>
<ul>
<li><p>We typically test <span class="math notranslate nohighlight">\(H_0: \mu_1 = \mu_2\)</span> (or <span class="math notranslate nohighlight">\(\mu_1 - \mu_2 = 0\)</span>).</p></li>
<li><p>Under <span class="math notranslate nohighlight">\(H_0\)</span>, <span class="math notranslate nohighlight">\(\mu_1 - \mu_2 = 0\)</span>, so the statistic simplifies to</p>
<div class="math notranslate nohighlight">
\[t \;=\; \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}.\]</div>
</li>
</ul>
<p><strong>Degrees of Freedom</strong> (<span class="math notranslate nohighlight">\(k\)</span>):</p>
<ul class="simple">
<li><p>Exact distribution is complicated.</p></li>
<li><p><strong>Software</strong> commonly uses the <strong>Satterthwaite approximation</strong>, which yields a <em>t</em> distribution with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom (not necessarily an integer).</p></li>
<li><p><strong>No software?</strong> A conservative approximation is <span class="math notranslate nohighlight">\(k = \min(n_1-1,\; n_2-1)\)</span>.</p></li>
</ul>
</div>
<input id="sd-tab-item-14" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" for="sd-tab-item-14">
Two-Sample <em>t</em> Confidence Interval</label><div class="sd-tab-content docutils">
<p><strong>3. Two-Sample <em>t</em> Confidence Interval</strong></p>
<p>A level <span class="math notranslate nohighlight">\(C\)</span> confidence interval for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[(\bar{x}_1 - \bar{x}_2) \;\pm\; t^* \,\sqrt{\frac{s_1^2}{n_1} \;+\; \frac{s_2^2}{n_2}},\]</div>
<p>where <span class="math notranslate nohighlight">\(t^*\)</span> is the critical value from the <span class="math notranslate nohighlight">\(t(k)\)</span> distribution cutting off an area of <span class="math notranslate nohighlight">\(\frac{1 - C}{2}\)</span> in each tail. The degrees of freedom <span class="math notranslate nohighlight">\(k\)</span> is approximated as above.</p>
<p><strong>Assumptions/Conditions</strong>:</p>
<ul class="simple">
<li><p><strong>Independent samples</strong>: The two samples must be independent of each other.</p></li>
<li><p><strong>Approximate Normality</strong>: For small samples, data from each population should be (roughly) Normal. For large <span class="math notranslate nohighlight">\(n_1, n_2\)</span>, the <em>t</em> procedures are robust.</p></li>
</ul>
<p><strong>4. Summary</strong></p>
<ul class="simple">
<li><p><strong>Two-sample <span class="math notranslate nohighlight">\(z\)</span> test</strong>: Rarely used because it requires <em>known</em> <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span>.</p></li>
<li><p><strong>Two-sample <em>t</em></strong>: Replaces unknown <span class="math notranslate nohighlight">\(\sigma_1, \sigma_2\)</span> with <span class="math notranslate nohighlight">\(s_1, s_2\)</span>.</p>
<ul>
<li><p>Hypothesis Tests: <span class="math notranslate nohighlight">\(H_0: \mu_1 = \mu_2 \quad \text{vs.}\quad H_a: \mu_1 \neq \mu_2\)</span> (or <span class="math notranslate nohighlight">\(&lt;\)</span>, <span class="math notranslate nohighlight">\(&gt;\)</span>).</p></li>
<li><p>Confidence Intervals: Estimate <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span> with an interval.</p></li>
<li><p>Degrees of Freedom: Usually approximated by software.</p></li>
</ul>
</li>
</ul>
<p>These procedures allow you to draw inferences on the difference between two population means, even if you do not know the actual population standard deviations.</p>
</div>
<input id="sd-tab-item-15" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" for="sd-tab-item-15">
Two-Sample <em>t</em> Significance Test</label><div class="sd-tab-content docutils">
<p><strong>1. Hypothesis and Test Statistic</strong></p>
<ul>
<li><p><strong>Hypotheses</strong>: Typically, we test</p>
<div class="math notranslate nohighlight">
\[  H_0: \mu_1 - \mu_2 = \Delta_0
  \quad\text{vs.}\quad
  H_a: \mu_1 - \mu_2 \neq \Delta_0
  \quad(\text{or } &lt;, &gt;).\]</div>
<p>A common case is <span class="math notranslate nohighlight">\(\Delta_0 = 0\)</span>.</p>
</li>
<li><p><strong>Two-Sample <em>t</em> Statistic</strong>:</p>
<div class="math notranslate nohighlight">
\[t 
= \frac{(\bar{x}_1 - \bar{x}_2) - \Delta_0}
       {\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}.\]</div>
<p>The degrees of freedom <span class="math notranslate nohighlight">\(\,k\)</span> are approximated (often by software using the Satterthwaite method).</p>
</li>
<li><p><strong><em>p</em>-Value</strong>: We use the <em>t</em><span class="math notranslate nohighlight">\((k)\)</span> distribution to find the <em>p</em>-value or critical value for the test.</p></li>
</ul>
<p><strong>2. Robustness and Sample Size Guidelines</strong></p>
<p>Just as with the one-sample <em>t</em>, these two-sample procedures are generally <strong>robust</strong> against moderate non-Normality.</p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(n_1 + n_2 = n_{\text{total}}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(n_{\text{total}} &lt; 15\)</span>, use <em>t</em> tests <strong>only</strong> if both samples look <em>reasonably</em> Normal (no major skew or outliers).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(15 \le n_{\text{total}} &lt; 40\)</span>, the <em>t</em> test works well unless you see outliers or strong skewness.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(n_{\text{total}} \ge 40\)</span>, the <em>t</em> methods are quite robust, even for skewed data.</p></li>
</ul>
<p><strong>Equal Sample Sizes</strong> are especially recommended for better robustness and more accurate <em>p</em>-values.</p>
<p><strong>3. Practical Tips</strong></p>
<ul class="simple">
<li><p><strong>Choosing Labels</strong>: You can label whichever sample as “population 1” or “population 2.” It often makes the test statistic positive, avoiding confusion about negative values of <span class="math notranslate nohighlight">\(t\)</span>.</p>
<ul>
<li><p><em>Important</em>: This is <strong>not</strong> the same as changing from a two-sided to one-sided test after seeing the data, which would be improper.</p></li>
</ul>
</li>
<li><p><strong>Small Samples</strong>: With very small <span class="math notranslate nohighlight">\(n_1\)</span> and <span class="math notranslate nohighlight">\(n_2\)</span>, we have limited power, and confidence intervals become wide. Even so, if an effect is large, it can still be detected with small samples, but proceed with caution when distributions are unknown or heavily skewed.</p></li>
</ul>
</div>
<input id="sd-tab-item-16" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" for="sd-tab-item-16">
Pooled Two-Sample <em>t</em> Procedures</label><div class="sd-tab-content docutils">
<p><strong>1. Key Assumption: Equal Standard Deviations</strong></p>
<ul class="simple">
<li><p>We assume both populations have the <em>same</em> (but unknown) standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>If true, we can combine (or <em>pool</em>) the two sample variances into a single estimate <span class="math notranslate nohighlight">\(s_p^2\)</span> to improve efficiency.</p></li>
</ul>
<p><strong>2. The Pooled Variance and <em>t</em> Statistic</strong></p>
<ol class="arabic simple">
<li><p><strong>Pooled Variance</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[s_p^2 
= \frac{(n_1 - 1)\,s_1^2 + (n_2 - 1)\,s_2^2}{n_1 + n_2 - 2}.\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Degrees of Freedom</strong>: <span class="math notranslate nohighlight">\(\;n_1 + n_2 - 2\)</span>.</p></li>
<li><p><strong>Pooled Two-Sample <em>t</em> Statistic</strong> for testing <span class="math notranslate nohighlight">\(H_0: \mu_1 - \mu_2 = \Delta_0\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[t 
= \frac{(\bar{x}_1 - \bar{x}_2) - \Delta_0}
      {s_p\,\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}.\]</div>
<ul class="simple">
<li><p>The corresponding confidence interval for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span> uses the same <span class="math notranslate nohighlight">\(s_p\)</span> and <em>t</em>-distribution with <span class="math notranslate nohighlight">\(n_1 + n_2 - 2\)</span> degrees of freedom.</p></li>
</ul>
<p><strong>3. Advantages and Caveats</strong></p>
<ul class="simple">
<li><p><strong>Advantages</strong>:</p>
<ul>
<li><p>Higher degrees of freedom than the general (unequal-<span class="math notranslate nohighlight">\(\sigma\)</span>) two-sample <em>t</em> test, which may yield slightly narrower confidence intervals and smaller <em>p</em>-values if the equal-<span class="math notranslate nohighlight">\(\sigma\)</span> assumption holds true.</p></li>
</ul>
</li>
<li><p><strong>Risks</strong>:</p>
<ul>
<li><p>The condition “<span class="math notranslate nohighlight">\(\sigma_1 = \sigma_2\)</span>” can be difficult to verify in practice.</p></li>
<li><p>If the population standard deviations differ substantially or the sample sizes are unbalanced, the pooled procedure can be <em>misleading</em>.</p></li>
<li><p>In modern practice, the <strong>unequal variance</strong> (unpooled) <em>t</em> approach is often safer-most software defaults to it (Satterthwaite approximation), unless you explicitly request pooling.</p></li>
</ul>
</li>
</ul>
<p><strong>4. Why Use a Pooled Estimator for <span class="math notranslate nohighlight">\(\sigma^2\)</span>?</strong></p>
<ul class="simple">
<li><p>The formula assigns <em>weights</em> proportional to the degrees of freedom in each sample, emphasizing the larger sample if <span class="math notranslate nohighlight">\(n_1 \neq n_2\)</span>.</p></li>
<li><p><strong>Benefits</strong>:</p>
<ul>
<li><p><strong>Higher Degrees of Freedom</strong>: The pooled procedure uses <span class="math notranslate nohighlight">\(n_1 + n_2 - 2\)</span> df, often larger than the approximate df from the unequal-variance (<em>Satterthwaite</em>) method.</p></li>
<li><p><strong>Efficiency</strong>: By combining variance estimates, we can get a more precise measure of the <em>common</em> <span class="math notranslate nohighlight">\(\sigma\)</span> when <span class="math notranslate nohighlight">\(\sigma_1 = \sigma_2\)</span> is truly valid.</p></li>
<li><p><strong>Narrower Intervals</strong>: Tends to give slightly smaller standard errors (and thus narrower confidence intervals) if the equal-variance assumption holds.</p></li>
</ul>
</li>
</ul>
</div>
</div>
</section>
<section id="sample-size-calculations">
<h2><span class="section-number">7.3. </span>Sample Size Calculations<a class="headerlink" href="#sample-size-calculations" title="Link to this heading">#</a></h2>
<p>For all the statistical procdeures we have introduced so far, besides population stadndard devaition <span class="math notranslate nohighlight">\(\sigma\)</span> and sample standard deviation <span class="math notranslate nohighlight">\(s\)</span>, we also see the sample size <span class="math notranslate nohighlight">\(n\)</span> in the formulas. In theory, this value is known to the researchers, but in practice, when we are planning a study, this is actually an important question, choosing the right number of sample size to make sure that the margin of error is less than a prespecified value <span class="math notranslate nohighlight">\(m\)</span> at a certain level of confidence. We know as sample size increase, our margin or error would decrease, however, increasing the sample size does not come free because we need to collect more data points. So that’ why we need to plan ahead to have large enough sample size to mmake sure we achieve <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>We start with the formula for margin of error</p>
<p>For all the <strong>statistical procedures</strong> we have introduced so far, in addition to the <strong>population standard deviation</strong> <strong><span class="math notranslate nohighlight">\(\sigma\)</span></strong> and <strong>sample standard deviation</strong> <strong><span class="math notranslate nohighlight">\(s\)</span></strong>, we also see the <strong>sample size</strong> <strong><span class="math notranslate nohighlight">\(n\)</span></strong> in the formulas.</p>
<p>In theory, the <strong>sample size</strong> <strong><span class="math notranslate nohighlight">\(n\)</span></strong> is known to researchers, but in <strong>practice</strong>, when planning a study, <strong>choosing the right sample size</strong> is an important decision. The goal is to ensure that the <strong>margin of error</strong> stays <strong>below a prespecified value</strong> <strong><span class="math notranslate nohighlight">\(m\)</span></strong> at a given <strong>confidence level</strong>.</p>
<p><strong>Trade-off: Precision vs. Cost</strong></p>
<ul class="simple">
<li><p>As the <strong>sample size increases</strong>, the <strong>margin of error decreases</strong>, improving <strong>precision</strong>.</p></li>
<li><p>However, <strong>collecting more data</strong> comes with increased <strong>time, cost, and effort</strong>.</p></li>
<li><p>This is why <strong>planning ahead</strong> is crucial: we need a <strong>large enough sample size</strong> to achieve the desired <strong>margin of error</strong> <strong><span class="math notranslate nohighlight">\(m\)</span></strong>, but not unnecessarily large to waste resources.</p></li>
</ul>
<p><strong>Starting with the Margin of Error Formula</strong></p>
<ul class="simple">
<li><p>To determine the required sample size, we begin with the formula for the <strong>margin of error</strong>, which depends on the confidence level and the variability in the population.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>1. Margin of Error for a Mean <span class="math notranslate nohighlight">\(\mu\)</span></strong></p>
<p>A one-sample <em>t</em> confidence interval has the margin of error:</p>
<div class="math notranslate nohighlight">
\[m = t^* \cdot \frac{s}{\sqrt{n}},\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the sample size,</p></li>
<li><p><span class="math notranslate nohighlight">\(t^*\)</span> is the <em>t</em>-critical value for the desired confidence level (depends on <span class="math notranslate nohighlight">\(\text{df} = n - 1\)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span> is the sample standard deviation <em>after</em> collecting data, but we <strong>guess</strong> a value <span class="math notranslate nohighlight">\(s_g\)</span> <em>beforehand</em> when planning.</p></li>
</ul>
<p><strong>Goal</strong>: Choose <span class="math notranslate nohighlight">\(n\)</span> to achieve an <em>expected</em> margin of error <span class="math notranslate nohighlight">\(\le m\)</span>.<br />
Since we won’t know the actual <span class="math notranslate nohighlight">\(s\)</span> until we sample, we use our best guess <span class="math notranslate nohighlight">\(s_g\)</span> for calculations.</p>
<ul class="simple">
<li><p>If previous studies or pilot data are available, we might estimate <span class="math notranslate nohighlight">\(\sigma\)</span> from those results.</p></li>
<li><p>In the absence of prior data, <strong>subject-matter expertise</strong> or known properties of the data can guide us.</p></li>
</ul>
<p>A simple rule of thumb:</p>
<div class="math notranslate nohighlight">
\[s_g = \frac{\text{range}}{4}.\]</div>
<ul class="simple">
<li><p><strong>Justification</strong>: For many roughly bell-shaped distributions, the range spans about 4 standard deviations (from roughly <span class="math notranslate nohighlight">\(\mu - 2\sigma\)</span> to <span class="math notranslate nohighlight">\(\mu + 2\sigma\)</span>), so dividing the range by 4 provides a crude estimate of <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
<p>Although not exact, it can be a useful starting point for sample-size or margin-of-error calculations when more precise estimates of <span class="math notranslate nohighlight">\(\sigma\)</span> are unavailable.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>2. Iterative Search Approach</strong></p>
<p>Because <span class="math notranslate nohighlight">\(t^*\)</span> itself depends on <span class="math notranslate nohighlight">\(n\)</span>, an <em>iterative</em> method is often used:</p>
<ol class="arabic simple">
<li><p><strong>Initial Approximation</strong>:</p></li>
</ol>
<ul>
<li><p>Replace <span class="math notranslate nohighlight">\(t^*\)</span> with the corresponding <em>z</em>-value (as if <span class="math notranslate nohighlight">\(\sigma\)</span> were known).</p></li>
<li><p>Solve</p>
<div class="math notranslate nohighlight">
\[  n = \Bigl(\frac{z^*\,s_g}{m}\Bigr)^2\]</div>
</li>
<li><p>and round up to the nearest integer.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Refine Using <em>t</em> Distribution</strong>:</p></li>
</ol>
<ul class="simple">
<li><p>With the tentative <span class="math notranslate nohighlight">\(n\)</span>, find the actual <span class="math notranslate nohighlight">\(t^*\)</span> for confidence level <span class="math notranslate nohighlight">\(C\)</span> and <span class="math notranslate nohighlight">\(\text{df} = n-1\)</span>.</p></li>
<li><p>Check if <span class="math notranslate nohighlight">\(m \ge t^* \cdot \frac{s_g}{\sqrt{n}}\)</span>.</p></li>
<li><p>If not met, increment <span class="math notranslate nohighlight">\(n\)</span> and repeat.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>This process continues until the requirement <span class="math notranslate nohighlight">\(m \ge t^*\frac{s_g}{\sqrt{n}}\)</span> is satisfied.</p></li>
</ol>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>3. Two-Sample Case</strong></p>
<ul>
<li><p>To design for a <strong>two-sample <em>t</em></strong> confidence interval for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>:</p>
<ul>
<li><p>Often assume equal sample sizes <span class="math notranslate nohighlight">\(n_1 = n_2 = n\)</span> and similar standard deviations <span class="math notranslate nohighlight">\(s_1 \approx s_2\)</span>.</p></li>
<li><p>The margin of error for <span class="math notranslate nohighlight">\(\bar{x}_1 - \bar{x}_2\)</span> becomes</p>
<div class="math notranslate nohighlight">
\[  m = t^* \cdot s_g \,\sqrt{\frac{1}{n} + \frac{1}{n}}
  \;=\;
  t^*\cdot s_g\sqrt{\frac{2}{n}}.\]</div>
</li>
<li><p>We again guess <span class="math notranslate nohighlight">\(s_g\)</span> and use an <em>iterative</em> method (now with <span class="math notranslate nohighlight">\(\text{df} \approx 2(n-1)\)</span>) if we need a more precise approach.</p></li>
</ul>
</li>
</ul>
</div>
<p>In addition to planning for the <strong>sample size</strong> <strong><span class="math notranslate nohighlight">\(n\)</span></strong>, we often also need to consider the <strong>power</strong> of our hypothesis test. These two concepts are <strong>closely related</strong>. Looking at the list of factors that influence power, we see that the first three also impact the <strong>margin of error</strong>.</p>
<ol class="arabic simple">
<li><p><strong>Significance Level <span class="math notranslate nohighlight">\(\alpha\)</span></strong></p>
<ul class="simple">
<li><p>A 5% test (<span class="math notranslate nohighlight">\(\alpha=0.05\)</span>) is more likely to reject <span class="math notranslate nohighlight">\(H_0\)</span> than a 1% test (<span class="math notranslate nohighlight">\(\alpha=0.01\)</span>) for the same true alternative, simply because <em>less</em> evidence is required to reject <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Population Standard Deviation <span class="math notranslate nohighlight">\(\sigma\)</span></strong></p>
<ul class="simple">
<li><p>More variability (larger <span class="math notranslate nohighlight">\(\sigma\)</span>) makes it harder to detect a true difference from <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
<li><p>With less variability, the test statistic is more sensitive to departures from <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Sample Size <span class="math notranslate nohighlight">\(n\)</span></strong></p>
<ul class="simple">
<li><p>Larger <span class="math notranslate nohighlight">\(n\)</span> reduces the standard error, making it easier to detect a given difference.</p></li>
<li><p>Power increases as <span class="math notranslate nohighlight">\(n\)</span> grows.</p></li>
</ul>
</li>
<li><p><strong>The Alternative (Effect Size)</strong></p>
<ul class="simple">
<li><p>The farther the true parameter is from the null hypothesis value, the easier it is to reject <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
<li><p>We often measure this distance as “<strong>effect size</strong>” = <span class="math notranslate nohighlight">\(\frac{\text{departure from }H_0}{\sigma}\)</span>.</p></li>
</ul>
</li>
</ol>
<ul class="simple">
<li><p>Before collecting data, researchers often choose:</p>
<ol class="arabic simple">
<li><p><strong><span class="math notranslate nohighlight">\(\alpha\)</span></strong>: The significance level (e.g., 5%).</p></li>
<li><p><strong>Minimum Detectable Effect</strong>: The smallest departure from <span class="math notranslate nohighlight">\(H_0\)</span> that matters in practice.</p></li>
<li><p><strong>Desired Power</strong> (e.g., 80% or 90%).</p></li>
<li><p><strong>Estimate of <span class="math notranslate nohighlight">\(\sigma\)</span></strong> (or <span class="math notranslate nohighlight">\(\sigma\)</span>s in two-sample scenarios).</p></li>
</ol>
</li>
<li><p><strong>Software</strong> is typically used to solve for the required <strong>sample size</strong> <span class="math notranslate nohighlight">\(n\)</span> that achieves the target power for detecting the specified effect size at level <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p><strong>Calculating Power</strong></p>
<ul>
<li><p><strong>Reject <span class="math notranslate nohighlight">\(H_0\)</span> Region</strong>: Determine the critical test statistic(s) that lead to rejection, based on <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p><strong>Assume a Specific <span class="math notranslate nohighlight">\(\mu\)</span> (or Effect Size)</strong> Under <span class="math notranslate nohighlight">\(H_a\)</span>: Find the probability that the test statistic falls in the rejection region, <em>given</em> this alternative is true.</p></li>
<li><p>Mathematically, this requires a <strong>noncentral t distribution</strong> (or other distribution, depending on the test). Manually this is tedious, so we rely on statistical software.</p></li>
</ul>
</li>
</ul>
</section>
<section id="unbiasedness-and-consistency-of-the-sample-standard-deviation">
<h2><span class="section-number">7.4. </span>Unbiasedness and Consistency of the Sample Standard Deviation<a class="headerlink" href="#unbiasedness-and-consistency-of-the-sample-standard-deviation" title="Link to this heading">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-17" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" for="sd-tab-item-17">
Unbiasedness of <span class="math notranslate nohighlight">\(s^2\)</span></label><div class="sd-tab-content docutils">
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. random variables with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.<br />
Define the <strong>sample variance</strong>:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2,
\quad\text{where}\quad
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i.\]</div>
<p><strong>Key Result</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[s^2] = \sigma^2\)</span>.<br />
Here’s the sketch of the proof:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[s^2]
&amp;= \mathbb{E}\!\Bigl[\frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\Bigr] \\[6pt]
&amp;= \frac{1}{n-1}\,\mathbb{E}\!\Bigl[\sum_{i=1}^n (X_i - \mu + \mu - \bar{X})^2\Bigr].
\end{aligned}\end{split}\]</div>
<p>One can expand <span class="math notranslate nohighlight">\((X_i - \bar{X})^2\)</span> into <span class="math notranslate nohighlight">\(\Bigl[(X_i - \mu) - (\bar{X} - \mu)\Bigr]^2\)</span> and use the fact that</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n (X_i - \bar{X})^2
= \sum_{i=1}^n (X_i - \mu)^2
 - n(\bar{X} - \mu)^2.\]</div>
<p>Taking expectations and using <span class="math notranslate nohighlight">\(\mathbb{E}[(\bar{X} - \mu)^2] = \frac{\sigma^2}{n}\)</span> yields</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\Bigl[\sum_{i=1}^n (X_i - \bar{X})^2\Bigr]
= \sum_{i=1}^n \mathbb{E}[(X_i - \mu)^2]
 - n\,\mathbb{E}[(\bar{X} - \mu)^2]
= n\,\sigma^2 - n\left(\frac{\sigma^2}{n}\right)
= (n-1)\,\sigma^2.\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[s^2]
= \frac{1}{n-1} \,\mathbb{E}\Bigl[\sum_{i=1}^n (X_i - \bar{X})^2\Bigr]
= \frac{1}{n-1} \cdot (n-1)\,\sigma^2
= \sigma^2.\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(s^2\)</span> is an <strong>unbiased estimator</strong> of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</div>
<input id="sd-tab-item-18" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" for="sd-tab-item-18">
Consistency of <span class="math notranslate nohighlight">\(s^2\)</span> and <span class="math notranslate nohighlight">\(s\)</span> (Optional)</label><div class="sd-tab-content docutils">
<p>To show <strong>consistency</strong>, we want:</p>
<div class="math notranslate nohighlight">
\[s^2 \xrightarrow{\;p\;} \sigma^2 \quad\text{(in probability)},
\quad \text{and} \quad
s \xrightarrow{\;p\;} \sigma.\]</div>
<ul>
<li><p>Consistency of <span class="math notranslate nohighlight">\(s^2\)</span></p>
<ul>
<li><p>By the <strong>Strong Law of Large Numbers</strong>,</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 \;\to\; \sigma^2 
\quad\text{almost surely.}\]</div>
</li>
<li><p>Observe that</p>
<div class="math notranslate nohighlight">
\[s^2 
= \frac{n}{n-1} \cdot \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2 
\approx \frac{n}{n-1} \cdot \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2,\]</div>
<p>where the second step uses <span class="math notranslate nohighlight">\(\bar{X} \approx \mu\)</span> for large <span class="math notranslate nohighlight">\(n\)</span>.</p>
</li>
<li><p>Since <span class="math notranslate nohighlight">\(\frac{n}{n-1} \to 1\)</span> and <span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 \to \sigma^2\)</span>, it follows that <span class="math notranslate nohighlight">\(s^2 \to \sigma^2\)</span> in probability (or almost surely under mild conditions). Thus, <span class="math notranslate nohighlight">\(s^2\)</span> is a <strong>consistent estimator</strong> of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
</ul>
</li>
<li><p>Consistency of <span class="math notranslate nohighlight">\(s\)</span></p>
<ul>
<li><p>By the <strong>Continuous Mapping Theorem</strong>, if <span class="math notranslate nohighlight">\(s^2 \xrightarrow{\;p\;} \sigma^2\)</span> and the square-root function <span class="math notranslate nohighlight">\(\sqrt{\cdot}\)</span> is continuous for <span class="math notranslate nohighlight">\(\sigma^2&gt;0\)</span>, then</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{s^2} \;\xrightarrow{\;p\;} \sqrt{\sigma^2} = \sigma.\]</div>
</li>
<li><p>Hence, <strong><span class="math notranslate nohighlight">\(s\)</span> is a consistent estimator of <span class="math notranslate nohighlight">\(\sigma\)</span></strong>.</p></li>
</ul>
</li>
</ul>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Chapter 6: Confidence Intervals and Significance Tests</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-for-the-mean-of-a-population">7.1. Inference for the Mean of a Population</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-two-means">7.2. Comparing Two Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-size-calculations">7.3. Sample Size Calculations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiasedness-and-consistency-of-the-sample-standard-deviation">7.4. Unbiasedness and Consistency of the Sample Standard Deviation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Frank (Chenzhong) Wu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>